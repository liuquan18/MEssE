! Copyright (c) 2024 The YAC Authors
!
! SPDX-License-Identifier: BSD-3-Clause

MODULE toy_atm

  USE mpi
  USE yac
  USE toy_common, ONLY : read_icon_grid, nsteps, define_fields, &
                         send_field, receive_field, &
                         max_char_length

  IMPLICIT NONE

  PRIVATE

  INTEGER :: t, ierror

  ! Basic string paramters
  CHARACTER(LEN=max_char_length), PARAMETER :: yaml_filename = "coupling.yaml"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_filename = "grids/icon_grid_0030_R02B03_G.nc"
  CHARACTER(LEN=max_char_length), PARAMETER :: comp_name = "atm_comp"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_name = "atm_grid"

  ! IDs and communicator generated by YAC
  INTEGER :: comp_id
  INTEGER :: comp_rank
  INTEGER :: comp_comm
  INTEGER :: grid_id
  INTEGER :: cell_point_id
  INTEGER :: field_taux_id
  INTEGER :: field_tauy_id
  INTEGER :: field_sfwflx_id
  INTEGER :: field_sftemp_id
  INTEGER :: field_thflx_id
  INTEGER :: field_iceatm_id
  INTEGER :: field_sst_id
  INTEGER :: field_oceanu_id
  INTEGER :: field_oceanv_id
  INTEGER :: field_iceoce_id

  ! Basic decomposed grid information
  INTEGER                       :: num_vertices
  INTEGER                       :: num_cells
  INTEGER                       :: num_vertices_per_cell
  INTEGER, ALLOCATABLE          :: cell_to_vertex(:,:)
  DOUBLE PRECISION, ALLOCATABLE :: x_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: x_cells(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_cells(:)
  INTEGER, ALLOCATABLE          :: cell_sea_land_mask(:)
  INTEGER, ALLOCATABLE          :: global_cell_id(:)

  ! Buffer for field data
  DOUBLE PRECISION, DIMENSION(:,:), ALLOCATABLE :: &
       taux, tauy, sfwflx, sftemp, thflx, iceatm, sst, oceanu, oceanv, iceoce

PUBLIC :: main_atm

CONTAINS

  SUBROUTINE main_atm(comm)

    INTEGER, INTENT(IN) :: comm

    comp_comm = comm

    ! Read coupling configuration file
    ! TODO

    ! Define local component
    ! TODO

    ! Retrieve communicator for ATM component
    ! TODO
    CALL MPI_Comm_rank(comp_comm, comp_rank, ierror)

    ! Read the grid and distribute it among all ATM processes
    CALL read_icon_grid( &
         grid_filename, comp_comm, cell_to_vertex, x_vertices, &
         y_vertices, x_cells, y_cells, cell_sea_land_mask, &
         global_cell_id)
    num_vertices = SIZE(x_vertices)
    num_cells = SIZE(x_cells)
    num_vertices_per_cell = SIZE(cell_to_vertex, 1)

    ! Define local part of the grid
    ! TODO

    ! Set global cell ids
    ! TODO

    ! Define location of the actual data (on cell centers)
    ! TODO

    !   ! Set mask for cell centers
    !   CALL yac_fset_mask(cell_sea_land_mask >= 0, cell_point_id)

    ! Define fields
    CALL define_fields( &
         comp_id, cell_point_id, field_taux_id, field_tauy_id, field_sfwflx_id, &
         field_sftemp_id, field_thflx_id, field_iceatm_id, field_sst_id, &
         field_oceanu_id, field_oceanv_id, field_iceoce_id)

    ! Complete definitions and compute interpolations
    ! TODO

    ! Initialise fields
    CALL init_fields()

    ! Execute model time loop
    DO t = 1, nsteps

       ! Simulate atm timestep
       CALL sim_atm_timestep(t)

       ! Exchange data with ocn component
       CALL couple_to_ocn()

    END DO ! time loop

  END SUBROUTINE main_atm

  SUBROUTINE init_fields()

    ! Allocate output field buffers
    ALLOCATE( &
      taux(num_cells, 2), tauy(num_cells, 2), sfwflx(num_cells, 3), &
      sftemp(num_cells, 1), thflx(num_cells, 4), iceatm(num_cells, 4))

    ! Initialise output field buffer with dummy data
    taux(:,1) = 10.1d0
    taux(:,2) = 10.2d0
    tauy(:,1) = 20.1d0
    tauy(:,2) = 20.2d0
    sfwflx(:,1) = 30.1d0
    sfwflx(:,2) = 30.2d0
    sfwflx(:,3) = 30.3d0
    sftemp(:,1) = 40.1d0
    thflx(:,1) = 50.1d0
    thflx(:,2) = 50.2d0
    thflx(:,3) = 50.3d0
    thflx(:,4) = 50.4d0
    iceatm(:,1) = 60.1d0
    iceatm(:,2) = 60.2d0
    iceatm(:,3) = 60.3d0
    iceatm(:,4) = 60.4d0

    ! Allocate input field buffers
    ALLOCATE( &
      sst(num_cells, 1), oceanu(num_cells, 1), oceanv(num_cells, 1), &
      iceoce(num_cells, 5))

    ! Initialise input field buffer with zero
    sst(:,1) = 0.0d0
    oceanu(:,1) = 0.0d0
    oceanv(:,1) = 0.0d0
    iceoce(:,1) = 0.0d0
    iceoce(:,2) = 0.0d0
    iceoce(:,3) = 0.0d0
    iceoce(:,4) = 0.0d0
    iceoce(:,5) = 0.0d0

  END SUBROUTINE init_fields

  SUBROUTINE sim_atm_timestep(timestep)

    INTEGER, INTENT(IN) :: timestep

    ! Do an atmosphere timestep

    ! ...

    CALL MPI_Barrier(comp_comm, ierror)
    IF (comp_rank == 0) &
      PRINT "('--- ',A3,' timestep ',I2,' ---')", comp_name, timestep

  END SUBROUTINE sim_atm_timestep

  SUBROUTINE couple_to_ocn()

    ! --------------------
    ! Send fields to ocean
    ! --------------------

    ! Send meridional wind stress
    CALL send_field(field_taux_id, taux)

    ! Send zonal  wind stress
    CALL send_field(field_tauy_id, tauy)

    ! Send surface fresh water flux
    CALL send_field(field_sfwflx_id, sfwflx)

    ! Send surface temperature
    CALL send_field(field_sftemp_id, sftemp)

    ! Send total heat flux
    CALL send_field(field_thflx_id, thflx)

    ! Send ice temperatures and melt potential
    CALL send_field(field_iceatm_id, iceatm)

    ! -------------------------
    ! Receive fields from ocean
    ! -------------------------

    ! Receive sea surface temperature
    CALL receive_field(comp_name, field_sst_id, sst)

    ! Recieve zonal velocity
    CALL receive_field(comp_name, field_oceanu_id, oceanu)

    ! Receive meridional velocity
    CALL receive_field(comp_name, field_oceanv_id, oceanv)

    ! Receive Ice thickness, concentration, T1 and T2
    CALL receive_field(comp_name, field_iceoce_id, iceoce)

  END SUBROUTINE couple_to_ocn

END MODULE toy_atm

PROGRAM main_atm_program
  USE mpi
  USE yac
  USE toy_atm, ONLY : main_atm

  IMPLICIT NONE

  INTEGER :: ierror

  ! Initialise MPI
  CALL MPI_Init(ierror)

  ! Initialise the YAC
  ! TODO

  ! Run atmosphere model
  CALL main_atm(MPI_COMM_WORLD)

  ! Finalise YAC
  ! TODO

  ! Finalise MPI
  CALL MPI_Finalize(ierror)

END PROGRAM main_atm_program
